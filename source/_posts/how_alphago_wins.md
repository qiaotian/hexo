title: AlphaGo如何思考
date: 2016-03-16
tag: ALPHAGO
---
**本文为译文，[网上有人分享了原文](https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf)**
**昨天，AlphaGo以4:1的比分击败李世石，mark了人工智能的又一伟大成就。本人忍不住抽时间读一读发表于Nature的那篇“说明书”。（不喜请狠拍=@=）**

**文章原标题是“通过深度神经网络和搜索树技术掌握围棋”。众所周知，围棋具有极大的搜索空间和局势评估复杂困难，因此围棋长期被认为是人工智能领域最具挑战性的传统棋类运动。这里Google引入了“数值网络”和“决策网络”两种方法分别用来评估棋局和选择棋路。AlphaGo的深度神经网络首先通过监督学习的全新组合方式，学习专业棋手的棋谱进行训练，然后通过与自己下棋不断强化训练。通过数千数万次的自我对战生成的蒙特卡洛树搜索程序，可以帮助AlphaGo在不需要任何前向搜索的情况下，其神经网络以当前蒙卡搜索树的水准进行对战。不仅如此，AlphaGo也引入了新的搜索算法，该算法将数值网络和决策网络与蒙卡模拟相结合在一起。使用该方法将AlphaGo对其它围棋程序的胜率提高到99.8%，并且给欧洲围棋冠军剃了光头（5:0）。这是第一次计算机程序在全幅对弈中战miao胜sha人类职业围棋选手。对于很多人而言，这一成就大大超前了。**

所有的完全信息博弈都有一个局面函数_v(x)_用于从当前局面或者状态下，并参考优秀选手的精妙走法后给比赛输出。这种博弈可以通过递归在搜索树中求解最优局面函数，如果我们假设_b_表示博弈的宽度（每个位置合法走法的数目），变量_b_表示博弈深度，那么这棵博弈树就包含_b^d_种可能的移动序列。对于一些小规模博弈，比如苏拉卡尔塔棋和爱因斯坦器，_b_和_d_都相对较小。而对于大型博弈游戏，比如国际象棋（_b_约为35，_d_约为80）和围棋（_b_约为250，_d_约为150），穷尽枚举在目前绝对是不可能的，但是可以通过两条通用原则缩小有效搜索空间。
1. 局面评估可以减小搜索深度。考虑到游戏复杂度，这种方法不适用与围棋。
2. 从决策分布_policy(a|s)_中抽样可以减小搜索宽度，其中_a_表示走法，_s_表示当前位置。


# 参考
https://zh.wikipedia.org/wiki/围棋
